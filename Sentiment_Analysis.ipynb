{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "mount_file_id": "1HmvnglzBgbOhivgOBpYQBtu1U1YC7jsZ",
      "authorship_tag": "ABX9TyPu2seU1SurlNwGfVue92jd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshakoul/Sentiment-Analysis-for-movie-reviews/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEkhBM4cdhbQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Build a classifier on IMDB movie dataset using a TF-IDF reresentation and logistic regression(and Naive Bayes)\n",
        "\n",
        "Taken data from: http://ai.stanford.edu/~amaas/data/sentiment/ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTDoabuODEAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "dirpath1 = ' ' #path where negative reviews are stored for training\n",
        "dirpath2 = ' ' #path where positive reviews are stored for training\n",
        "output = 'output_file.csv'\n",
        "with open(output, 'w') as outfile:\n",
        "    csvout = csv.writer(outfile)\n",
        "    csvout.writerow(['senti', 'review'])\n",
        "\n",
        "    files = os.listdir(dirpath1)\n",
        "\n",
        "    for filename in files:\n",
        "        with open(dirpath1 + '/' + filename) as afile:\n",
        "            csvout.writerow([0, afile.read()])\n",
        "            afile.close()\n",
        "\n",
        "    files = os.listdir(dirpath2)\n",
        "\n",
        "    for filename in files:\n",
        "        with open(dirpath2 + '/' + filename) as afile:\n",
        "            csvout.writerow([1, afile.read()])\n",
        "            afile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFmwUTbcKGO_",
        "colab_type": "text"
      },
      "source": [
        "# **Text - processing**\n",
        "* convert to lower case\n",
        "* get rid of ascents\n",
        "* tokenization\n",
        "* create vocab to index\n",
        "* replace words into numbers: Encoding the reviews\n",
        "* Encode the labels  -- already done as per script combine.py\n",
        "    \n",
        "    [‘positive’ as 1 and ‘negative’ as 0]\n",
        "\n",
        "**Note:** \n",
        "\n",
        "To convert text docs to numbers we can use foll. techniques:\n",
        "\n",
        "1.   CountVectorizer\n",
        "2.   TfidfVectorizer\n",
        "3.   HashingVectorizer \n",
        "\n",
        "In this I have used TfidfVectorizer from sklearn, this module takes care of removal of stop words, ascents, lower case and also tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfjcufMYSErY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U nltk==3.4\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "              for category in movie_reviews.categories()\n",
        "              for fileid in movie_reviews.fileids(category)]\n",
        "#movie_reviews.fileids(\"pos\") --> pos/cv937_9811.txt\n",
        "#documents is  a list, each element in list is a tuple\n",
        "#tuple : ( list1 , string) where list1 is a list of strings(words in review)\n",
        "#A clear eample is documents[0][0][0] --> first review, first word \n",
        "# documents[0][1] --> pos or neg of first review\n",
        "# /content/drive/My Drive/iisc/summer/datasets/IMDB Dataset.csv              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT4_0XMngLGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0hqdynXVCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/train.csv')\n",
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiUmvTnTtWXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/test.csv')\n",
        "df_test = df_test.sample(frac=1).reset_index(drop=True) #shuffle rows"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa_bGVM6qdIe",
        "colab_type": "text"
      },
      "source": [
        "**Note**: No tokenization has been done as that will be handled by tf-idf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mzrDR92fnYe",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF** : converting text to words to integers\n",
        "\n",
        "**TF:**The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n",
        "![alt text](https://miro.medium.com/proxy/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
        "\n",
        "\n",
        "**IDF:**The log of the number of documents divided by the number of documents that contain the word w. It determines the weight of rare words across all documents in the corpus.\n",
        "\n",
        "![alt text](https://miro.medium.com/proxy/1*A5YGwFpcTd0YTCdgoiHFUw.png)\n",
        "\n",
        "\n",
        "Lastly, the TF-IDF is simply the TF multiplied by IDF.\n",
        "![alt text](https://miro.medium.com/proxy/1*nSqHXwOIJ2fa_EFLTh5KYw.png)\n",
        "\n",
        "**TfidfVectorizer** class provided by **sklearn** : Convert a collection of raw documents to a matrix of TF-IDF features. It will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.\n",
        "\n",
        "This is equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "\n",
        "\n",
        "**NOTE:** to make sure that both train and test have same dimemsnions for tfidf matrix we need to ::\n",
        "[SO](https://stackoverflow.com/questions/39170169/identical-dimensions-for-train-and-test-matrices-in-text-analysis)\n",
        "\n",
        "train_X = vectorizer.fit_transform(train)\n",
        "\n",
        "test_X = vectorizer.transform(test)\n",
        "\n",
        "or\n",
        "\n",
        "vectorizer.fit_transform([train, test])\n",
        "\n",
        "Using transform instead of fit_transform preserves the vocabulary created from fit_transform in the previous line, and ensures identical columns for these matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqXeHJYTdYhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Without going into the math, TF-IDF are word frequency scores that try to highlight\n",
        "# words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_model = TfidfVectorizer(lowercase=True, strip_accents='ascii', analyzer='word',stop_words='english') #matrix returned is by default float64 \n",
        "\n",
        "tv_train_reviews=vectorizer_model.fit_transform(df[\"review\"])\n",
        "#transformed test reviews\n",
        "tv_test_reviews=vectorizer_model.transform(df_test[\"review\"])\n",
        "#Now document-term matrix \"tv_train_reviews\" has the TF-IDF values of all the documents in the corpus. This is a big sparse matrix.\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dAWPj2D1-fh",
        "colab_type": "text"
      },
      "source": [
        "Simple logistic regression technique using TFIDF representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7LObAYujrbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71bdc98f-3cbc-44f1-e13a-af70f2a4fe5b"
      },
      "source": [
        "print(tv_train_reviews.shape) \n",
        "#MODELLING \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "lr.fit(tv_train_reviews, df['senti'])\n",
        "\n",
        "#predict using test data\n",
        "test_predicted = lr.predict(tv_test_reviews)\n",
        "score = accuracy_score(df_test[\"senti\"],test_predicted)\n",
        "print(score) #87"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 74515)\n",
            "0.87928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6P3z1oT13Cz",
        "colab_type": "text"
      },
      "source": [
        "Multinomial Naive Bayes using TFIDF representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhdnqP_exNKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad0fa3b9-262b-4e93-d7e4-a2b3d0e6b510"
      },
      "source": [
        "#training the model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "nb_tf = nb.fit(tv_train_reviews, df['senti'])\n",
        "#predict using test data\n",
        "test_predicted = nb.predict(tv_test_reviews)\n",
        "score = accuracy_score(df_test[\"senti\"],test_predicted)\n",
        "print(score)  #83"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.82992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFZfen9L5WsX",
        "colab_type": "text"
      },
      "source": [
        "**Do Text Cleaning on my own to verify results:**\n",
        "\n",
        "*   lower case\n",
        "*   Stopwords removal\n",
        "*   special chars removal\n",
        "*   text stemming\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmNlnra5V44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e69d13c4-9d13-45fe-e8c7-d2b30e844b08"
      },
      "source": [
        "import nltk\n",
        "\n",
        "#lower cases\n",
        "df['review'] = df['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "\n",
        "#Punctuations\n",
        "import string\n",
        "def rem_punctuation(text):\n",
        "  translator = str.maketrans('','',string.punctuation) #!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "  modified_text = text.translate(translator)\n",
        "  return modified_text;\n",
        "\n",
        "#remove special charcters\n",
        "import re\n",
        "def rem_special_characters(text, remove_digits=True):\n",
        "    text = re.sub(r'<.*?>',\"\",text) #remove html tags/urls\n",
        "    pattern=r'[^a-zA-z0-9\\s]' #remove evrythig except\n",
        "    text = re.sub(pattern,'',text) \n",
        "    return text\n",
        "\n",
        "#remove stopwords    \n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  nostop_tokens =  [i for i in tokens if i not in stop];\n",
        "  text  = \" \".join(nostop_tokens)\n",
        "  return text\n",
        "\n",
        "\n",
        "#Text Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "def text_stemmer(text):\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  tokens = word_tokenize(text)\n",
        "  text = \" \".join([stemmer.stem(token) for token in tokens]) \n",
        "  return text  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv3VFFJd6zEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review'] = df['review'].apply(text_stemmer)  \n",
        "df['review'] = df['review'].apply(rem_special_characters)\n",
        "df['review'] = df['review'].apply(remove_stopwords)\n",
        "df_test['review'] = df_test['review'].apply(text_stemmer)  \n",
        "df_test['review'] = df_test['review'].apply(rem_special_characters)\n",
        "df_test['review'] = df_test['review'].apply(remove_stopwords)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llabCWKn61XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tv_train_reviews=vectorizer.fit_transform(df[\"review\"])\n",
        "tv_test_reviews=vectorizer.transform(df_test[\"review\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS17mXJJ7d97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7b0c6717-4a7e-4459-ebaa-cea2fa3c7f8f"
      },
      "source": [
        "print(tv_train_reviews.shape) \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "lr.fit(tv_train_reviews, df['senti'])\n",
        "\n",
        "#predict using test data\n",
        "test_predicted = lr.predict(tv_test_reviews)\n",
        "score = accuracy_score(df_test[\"senti\"],test_predicted)\n",
        "print(score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 75527)\n",
            "0.88132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OvQ3WCHzhGy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n"
      ]
    }
  ]
}